There were some difficulties up till now, see commits on branch goes on github.
Problems were with adapting GNU-MAGIC to work with GOES. In the end the data was
converted from netCDF to a binary format on a regular lat/lon grid and then
worked on.

-- 20.11.2012
Success! The whole thing's running - see the commits from the last four days of 
so on GNU-MAGIC.
I've got the regridding going on the server at the moment and will start in on 
regridding the rest of the stuff simultaneously.
In order to work with the results:

1. Got to read the XPIF format. The easiest way I've found thus far is to cut 
off the header (256 bytes). I do that like this:
dd if=infile of=outfile bs=1 skip=256
That just chops off the first 256 bits and throws them out.

This could also be accomplished pretty easily by using python. You can skip to
a certain byte position with the following code (example):
# Open file object f at specified path in read mode
# I haven't tested this, so the mode might also need "b" for binary.
# Apparently, it's irrelevant in Unix systems.
f = open("/path/to/file.out", "r")
# Go to 256th byte in the file
f.seek(256)
# Read the rest of the file and store it in variable var
var = f.read()
# Then you'd need to write it to another file

2. After that I like to read the stuff in GRASS. No problem, the data's 
basically the same as an Idrisi file. So make an Idrisi file with the right 
specs. Here's an example file:
-- start file --

***********

file format : IDRISI Raster A.1
file title  : Example sneaky XPIF import
data type   : byte
file type   : binary
columns     : 4251
rows        : 6201
ref. system : plane
ref. units  : m
unit dist.  : 1.0000000
min. X      : 95
max. X      : 180
min. Y      : -62
max. Y      : 62
pos`n error : unknown
resolution  : unknown
min. value  : 0
max. value  : 255
display min : 0
display max : 255
value units : unspecified
value error : unknown
flag value  : none
flag def`n  : none
legend cats : 0

*************

-- end file --
The header file is stored as filename.rdc, whereas the actual data has the name
filename.rst. It's important that they're named the same.
Make sure the coordinates, as well as the column and row numbers are correct.

3. Import it into GRASS. This is easiest with a r.in.gdal. Importing the map
above into WGS84/ll gives me a picture that looks like it has the right 
coordinates and stuff. I have to override the map#s projection check though to
get GRASS to play with it. Example command:
r.in.gdal input=infile.rst output=test -o

Soon I should write a program that takes care of that, but for now I'm just 
going to leave it alone.

Another note: After processing a bunch of files, I noticed that I was missing
several GOES-W time steps. The reason for this is that the xdim and ydim of the
netCDF files are different for the time steps > 9 UTC. At the end of the GOES-W
regridding process I need to set it up again for those pictures with irregular
dimensions. That's also relevant for importing CAL into GRASS later on. Here's
the dimensions of the (normal) satellite coverages. Local noon was calculated as
local_noon = 13 UTC - satellite.degrees_east / 15
You've got to try to get as close as possible to local noon for normalization.

GOES-W:
-62° - 62° N
-180° - -75° E
5251 x 6201 pixels
Local noon: 22 UTC
# 99 files matched these dimensions. They were moved into folder 5251x6201
Secondary data:
# This data had different xc and yc numbers and were processed separately.
-62° - 62° N
-180° - -75° E
5255 x 6205
Local noon: 22 UTC

GOES-E:
-62° - 62° N
-134° - -14° E
5851 x 6201 pixels
Local noon: 18 UTC

GMS:
-62° - 62° N
95° - 180° E
4251 x 6201 pixels
Local noon: 3 UTC

-- 23.11.2012
Made a text file to tell which GOES-W data had which number of rows and columns.
It's stored in 062003_regridded.

-- 29.11.2012
It'll be a fun time getting the scan column. I'm looking at using the following
formula:

measure_time = scan_start + (column / time_per_clumn) + (row / row_length *
        time_per_column)

Probably the easiest way of dealing with that is to:
* Read the ncdump of one of the original files with Python
* Split it into lat, lon, and z lists
* Find the position in the array with the right coordinates
* Then: time = scan_start + (scan_time * (array_position / array_length))

That should do the trick.
