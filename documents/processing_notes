There were some difficulties up till now, see commits on branch goes on github.
Problems were with adapting GNU-MAGIC to work with GOES. In the end the data was
converted from netCDF to a binary format on a regular lat/lon grid and then
worked on.

-- 20.11.2012
Success! The whole thing's running - see the commits from the last four days of 
so on GNU-MAGIC.
I've got the regridding going on the server at the moment and will start in on 
regridding the rest of the stuff simultaneously.
In order to work with the results:

1. Got to read the XPIF format. The easiest way I've found thus far is to cut 
off the header (256 bytes). I do that like this:
dd if=infile of=outfile bs=1 skip=256
That just chops off the first 256 bits and throws them out.

This could also be accomplished pretty easily by using python. You can skip to
a certain byte position with the following code (example):
# Open file object f at specified path in read mode
# I haven't tested this, so the mode might also need "b" for binary.
# Apparently, it's irrelevant in Unix systems.
f = open("/path/to/file.out", "r")
# Go to 256th byte in the file
f.seek(256)
# Read the rest of the file and store it in variable var
var = f.read()
# Then you'd need to write it to another file

2. After that I like to read the stuff in GRASS. No problem, the data's 
basically the same as an Idrisi file. So make an Idrisi file with the right 
specs. Here's an example file:
-- start file --

***********

file format : IDRISI Raster A.1
file title  : Example sneaky XPIF import
data type   : byte
file type   : binary
columns     : 4251
rows        : 6201
ref. system : plane
ref. units  : m
unit dist.  : 1.0000000
min. X      : 95
max. X      : 180
min. Y      : -62
max. Y      : 62
pos`n error : unknown
resolution  : unknown
min. value  : 0
max. value  : 255
display min : 0
display max : 255
value units : unspecified
value error : unknown
flag value  : none
flag def`n  : none
legend cats : 0

*************

-- end file --
The header file is stored as filename.rdc, whereas the actual data has the name
filename.rst. It's important that they're named the same.
Make sure the coordinates, as well as the column and row numbers are correct.

3. Import it into GRASS. This is easiest with a r.in.gdal. Importing the map
above into WGS84/ll gives me a picture that looks like it has the right 
coordinates and stuff. I have to override the map#s projection check though to
get GRASS to play with it. Example command:
r.in.gdal input=infile.rst output=test -o

Soon I should write a program that takes care of that, but for now I'm just 
going to leave it alone.

Another note: After processing a bunch of files, I noticed that I was missing
several GOES-W time steps. The reason for this is that the xdim and ydim of the
netCDF files are different for the time steps > 9 UTC. At the end of the GOES-W
regridding process I need to set it up again for those pictures with irregular
dimensions. That's also relevant for importing CAL into GRASS later on. Here's
the dimensions of the (normal) satellite coverages. Local noon was calculated as
local_noon = 13 UTC - satellite.degrees_east / 15
You've got to try to get as close as possible to local noon for normalization.

GOES-W:
-62° - 62° N
-180° - -75° E
5251 x 6201 pixels
Local noon: 22 UTC
# 99 files matched these dimensions. They were moved into folder 5251x6201
Secondary data:
# This data had different xc and yc numbers and were processed separately.
-62° - 62° N
-180° - -75° E
5255 x 6205
Local noon: 22 UTC

GOES-E:
-62° - 62° N
-134° - -14° E
5851 x 6201 pixels
Local noon: 18 UTC

GMS:
-62° - 62° N
95° - 180° E
4251 x 6201 pixels
Local noon: 3 UTC

-- 23.11.2012
Made a text file to tell which GOES-W data had which number of rows and columns.
It's stored in 062003_regridded.

-- 29.11.2012
It'll be a fun time getting the scan column. I'm looking at using the following
formula:

measure_time = scan_start + (column / time_per_clumn) + (row / row_length *
        time_per_column)

Probably the easiest way of dealing with that is to:
* Read the ncdump of one of the original files with Python
* Split it into lat, lon, and z lists
* Find the position in the array with the right coordinates
* Then: time = scan_start + (scan_time * (array_position / array_length))

That should do the trick.

-- 30.11.2012
Started GOES-E and -W again. The problem was that I'd forgotten to specify the
correct longitude in the arguments passed to helclim.exe. Oops. Outputs look
fine so far now, I'll let it run over the weekend.

-- 1.12.2012
One advantage of magicsolv0-goes.exe is that it takes the lowest number of
pixels - that means that the outputs all have the same dimensions, I don't have
any of the trouble with having to convert pictures with different dimensions
into viewable PGMs.

-- 5.12.2012
Tried a bunch of times to get allsky running on the server, but it didn't work.
For some reason, only the upper parts of the picture are computed. I believe
that it's got something to do with 32-bit/64-bit conflicts. I can't compile in
32-bit mode on that computer because the header files are missing, so I tried:
    1. Compiling in 64-bit mode anyway. Bad results.
    2. Compiling in 32-bit mode on my computer and putting the 32-bit binary on
    the cluster. Bad news - the code seems to use some kind of dynamic linking,
    so no dice. It ran, but gave me bad results.
    3. Running from my computer while accessing the files on the server. For
    some reason, I was unable to write outputs.
    4. Copying cloud albedo files to my computer and running it there. That
    seems to be working, tests are in progress.

-- 7.12.2012
It's done! Now I just need to get GRASS working on the server, because for some
reason GRASS won't write to a mapset on the server, even though I've given
myself the permissions. I've asked Spaska to install GRASS there.

*Allocating satellite footprints*
Now I've got to figure out which footprints I take of each area. I'm going to be
working on the satellite imagery with cdos, so I need to be able to cut them up
before I merge them.

I'm using the following algorithm to find borders, always looking for the right
border:
{
    Satellite positions (°E):
        GOES-W:     -135
        GOES-E:     -74
        MSat-Prime: 0
        MSat-IODC:  57.5
        GMS:        55
    
    domain_satellite_west - satellite_west < satellite_east - domain_satellite_west
        --> 2 * domain_satellite_west < satellite_east + satellite_west
        --> domain_satellite_west < (satellite_east + satellite_west) / 2
}
This yields e.g. for the border between GOES-W/GOES-E:
domain_satellite_west - -135 < -74 - domain_satellite_west
2 * domain_satellite_west < -74 - 135
2 * domain_satellite_west < -209
domain_satellite_west < -104.5

That means I've got the following satellite E/W domains:
GOES-W:     -180° - -134°
GOES-E:     -134° - -37°
MSat-Prime:  -37° - 29° (rounded)
MSat-IODC:    29° - 106° (rounded)
GMS:         106° - 180°

Why -134° for the GOES-W/GOES-E border? My data is way cut off because I was 
using Richard's stupid test coordinates. I am so dumb. The minimum longitude of
GMS is 74, but I was cutting things off at 90. That isn't a problem EXCEPT for
with the GOES-E data, which I very intelligently cut off at -134°.

So I can divvy that stuff up now into their nice little slices and then merge
them.
I plan on combining the closest time steps to get as close as I can to an
instantaneous product. Nonetheless, this three-hour product will be aggregated
down below what the other satellites can do, so I'll also validate the
satellites individually.

-- 11.12.2012
Made some real progress, I think. I installed the cdos on the server - 
~/data/lib/cdo/bin/cdo. It's got netCDF support as well. And now I've got the 
rudimentaries for the rest of the stuff I'll be doing:
1. Extract individual variables from the netCDF files:
cdo splitname ifile oprefix
2. Translate netCDFs to TIFFs for mapping
gdal_translate ifile ofile.ext
3. Merge them together
gdal_merge.py ifile1 ifile2 -o mergedfile

The nice thing is that I can read out the station data directly with R (put a 
script in my script folder). So all I need to do is load the stations into my
R environment and then pass the points to the query to get them from each
file. A little bit of experimentation should take care of everything there - 
I'm thinking of just splitting the points into the different coverage areas
and passing them like that. Should work like a charm!