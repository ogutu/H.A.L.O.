>>>>>>>>taken care of
There were some difficulties up till now, see commits on branch goes on github.
Problems were with adapting GNU-MAGIC to work with GOES. In the end the data was
converted from netCDF to a binary format on a regular lat/lon grid and then
worked on.

-- 20.11.2012
Success! The whole thing's running - see the commits from the last four days of 
so on GNU-MAGIC.
I've got the regridding going on the server at the moment and will start in on 
regridding the rest of the stuff simultaneously.
In order to work with the results:

1. Got to read the XPIF format. The easiest way I've found thus far is to cut 
off the header (256 bytes). I do that like this:
dd if=infile of=outfile bs=1 skip=256
That just chops off the first 256 bits and throws them out.

This could also be accomplished pretty easily by using python. You can skip to
a certain byte position with the following code (example):
# Open file object f at specified path in read mode
# I haven't tested this, so the mode might also need "b" for binary.
# Apparently, it's irrelevant in Unix systems.
f = open("/path/to/file.out", "r")
# Go to 256th byte in the file
f.seek(256)
# Read the rest of the file and store it in variable var
var = f.read()
# Then you'd need to write it to another file

2. After that I like to read the stuff in GRASS. No problem, the data's 
basically the same as an Idrisi file. So make an Idrisi file with the right 
specs. Here's an example file:
-- start file --

***********

file format : IDRISI Raster A.1
file title  : Example sneaky XPIF import
data type   : byte
file type   : binary
columns     : 4251
rows        : 6201
ref. system : plane
ref. units  : m
unit dist.  : 1.0000000
min. X      : 95
max. X      : 180
min. Y      : -62
max. Y      : 62
pos`n error : unknown
resolution  : unknown
min. value  : 0
max. value  : 255
display min : 0
display max : 255
value units : unspecified
value error : unknown
flag value  : none
flag def`n  : none
legend cats : 0

*************

-- end file --
The header file is stored as filename.rdc, whereas the actual data has the name
filename.rst. It's important that they're named the same.
Make sure the coordinates, as well as the column and row numbers are correct.

3. Import it into GRASS. This is easiest with a r.in.gdal. Importing the map
above into WGS84/ll gives me a picture that looks like it has the right 
coordinates and stuff. I have to override the map#s projection check though to
get GRASS to play with it. Example command:
r.in.gdal input=infile.rst output=test -o

Soon I should write a program that takes care of that, but for now I'm just 
going to leave it alone.

Another note: After processing a bunch of files, I noticed that I was missing
several GOES-W time steps. The reason for this is that the xdim and ydim of the
netCDF files are different for the time steps > 9 UTC. At the end of the GOES-W
regridding process I need to set it up again for those pictures with irregular
dimensions. That's also relevant for importing CAL into GRASS later on. Here's
the dimensions of the (normal) satellite coverages. Local noon was calculated as
local_noon = 13 UTC - satellite.degrees_east / 15
You've got to try to get as close as possible to local noon for normalization.

GOES-W:
-62° - 62° N
-180° - -75° E
5251 x 6201 pixels
Local noon: 22 UTC
# 99 files matched these dimensions. They were moved into folder 5251x6201
Secondary data:
# This data had different xc and yc numbers and were processed separately.
-62° - 62° N
-180° - -75° E
5255 x 6205
Local noon: 22 UTC

GOES-E:
-62° - 62° N
-134° - -14° E
5851 x 6201 pixels
Local noon: 18 UTC

GMS:
-62° - 62° N
95° - 180° E
4251 x 6201 pixels
Local noon: 3 UTC

-- 23.11.2012
Made a text file to tell which GOES-W data had which number of rows and columns.
It's stored in 062003_regridded.

-- 29.11.2012
It'll be a fun time getting the scan column. I'm looking at using the following
formula:

measure_time = scan_start + (column / time_per_clumn) + (row / row_length *
        time_per_column)

Probably the easiest way of dealing with that is to:
* Read the ncdump of one of the original files with Python
* Split it into lat, lon, and z lists
* Find the position in the array with the right coordinates
* Then: time = scan_start + (scan_time * (array_position / array_length))

That should do the trick.

-- 30.11.2012
Started GOES-E and -W again. The problem was that I'd forgotten to specify the
correct longitude in the arguments passed to helclim.exe. Oops. Outputs look
fine so far now, I'll let it run over the weekend.

-- 1.12.2012
One advantage of magicsolv0-goes.exe is that it takes the lowest number of
pixels - that means that the outputs all have the same dimensions, I don't have
any of the trouble with having to convert pictures with different dimensions
into viewable PGMs.

-- 5.12.2012
Tried a bunch of times to get allsky running on the server, but it didn't work.
For some reason, only the upper parts of the picture are computed. I believe
that it's got something to do with 32-bit/64-bit conflicts. I can't compile in
32-bit mode on that computer because the header files are missing, so I tried:
    1. Compiling in 64-bit mode anyway. Bad results.
    2. Compiling in 32-bit mode on my computer and putting the 32-bit binary on
    the cluster. Bad news - the code seems to use some kind of dynamic linking,
    so no dice. It ran, but gave me bad results.
    3. Running from my computer while accessing the files on the server. For
    some reason, I was unable to write outputs.
    4. Copying cloud albedo files to my computer and running it there. That
    seems to be working, tests are in progress.

Got it running in the end by recompiling the code locally and moving it to the
server. There was a problem in the control script - the program was reading 2
bytes per pixel rather than 1. Fixed that and everything worked fine. Here are
the times needed (computation was performed simultaneously):
GOES-W: 20.5h
GOES-E: 22.6h
GMS:    43.3h

-- 7.12.2012
It's done! Now I just need to get GRASS working on the server, because for some
reason GRASS won't write to a mapset on the server, even though I've given
myself the permissions. I've asked Spaska to install GRASS there.

*Allocating satellite footprints*
Now I've got to figure out which footprints I take of each area. I'm going to be
working on the satellite imagery with cdos, so I need to be able to cut them up
before I merge them.

I'm using the following algorithm to find borders, always looking for the right
border:
{
    Satellite positions (°E):
        GOES-W:     -135
        GOES-E:     -74
        MSat-Prime: 0
        MSat-IODC:  57.5
        GMS:        155
    
    domain_satellite_west - satellite_west < satellite_east - domain_satellite_west
        --> 2 * domain_satellite_west < satellite_east + satellite_west
        --> domain_satellite_west < (satellite_east + satellite_west) / 2
}
This yields e.g. for the border between GOES-W/GOES-E:
domain_satellite_west - -135 < -74 - domain_satellite_west
2 * domain_satellite_west < -74 - 135
2 * domain_satellite_west < -209
domain_satellite_west < -104.5

That means I've got the following satellite E/W domains:
GOES-W:     -180° - -134°
GOES-E:     -134° - -37°
MSat-Prime:  -37° - 29° (rounded)
MSat-IODC:    29° - 106° (rounded)
GMS:         106° - 180°

Why -134° for the GOES-W/GOES-E border? My data is way cut off because I was 
using Richard's stupid test coordinates. I am so dumb. The minimum longitude of
GMS is 74, but I was cutting things off at 90. That isn't a problem EXCEPT for
with the GOES-E data, which I very intelligently cut off at -134°.

So I can divvy that stuff up now into their nice little slices and then merge
them.
I plan on combining the closest time steps to get as close as I can to an
instantaneous product. Nonetheless, this three-hour product will be aggregated
down below what the other satellites can do, so I'll also validate the
satellites individually.

-- 11.12.2012
Made some real progress, I think. I installed the cdos on the server - 
~/data/lib/cdo/bin/cdo. It's got netCDF support as well. And now I've got the 
rudimentaries for the rest of the stuff I'll be doing:
1. Extract individual variables from the netCDF files:
cdo splitname ifile oprefix
2. Translate netCDFs to TIFFs for mapping
gdal_translate ifile ofile.ext
3. Merge them together
gdal_merge.py ifile1 ifile2 -o mergedfile

The nice thing is that I can read out the station data directly with R (put a 
script in my script folder). So all I need to do is load the stations into my
R environment and then pass the points to the query to get them from each
file. A little bit of experimentation should take care of everything there - 
I'm thinking of just splitting the points into the different coverage areas
and passing them like that. Should work like a charm!

-- 20.12.2012
All of the data is homogenized but I still haven't made merged pictures of the
entire world. Got to get to that. I'm also missing the SIS data from Meteosat
but I've ordered it. Just need to export it as I did the SID / CAL data.

========

Jörg suggested that I compare the cloud albedo or the irradiation to a MODIS
cloud mask. He also suggested that I compare the consistence of GOES and
Meteosat pictures. This creates the following two goals:

1. Compare cloud albedo to MODIS:
    * Download MODIS cloud masks
        I've started this and decided to do quite a bit of comparing. Each
        satellite needs to be validated. I'm going to take daytime slots right in
        the middle of each satellite's scan at noon, if possible. Those are:
        --
        GOES-W: sunrise 14:58, noon 21:01, sunset 3:05, three-hourly scans at 0000
        GOES-E: sunrise 10:54, noon 16:57, sunset 23:01, three-hourly scans at 0245
        Msat-Prime: sunrise 5:58, noon 12:01, sunset 18:05, hourly scans at
            0000
        Msat-IODC: sunrise 2:08, noon 8:11, sunset 14:15, hourly scans at
            0000
        GMS: sunrise 19:38, noon 1:41, sunset 7:45, hourly scans at 0025
        --
        For the days 152, 166 and 181 (beginning, middle & end of month) I've
        got a noontime scan (inside of 3 hours) from somewhere in the area of
        each satellite.

        I took Aqua because it crosses the equator at about noon so I figured it
        would give us the best coverage without any possible paralax errors. I
        always took the daytime scan in the middle of or closest to the
        satellite's midday scan.

        I ordered everything and had it preprocessed to GeoTIFFs, hopefully
        it'll get here soon and I can take a look at it.
    * Resample them to WGS84 w/ proper resolution
        The data I ordered has a resolution of 1 km, no matter where it is. The
        MAGICSOL data has a lower resolution - eventually I would do the whole
        thing with higher res, but why put all your cats in one barn, right? Is
        that even a saying? What the hell's going on????
        ---
        #1#27297583972
        overflow.stack_cap.weihnachten: We apologize for the short service
        interruption. Your worker unit has been disabled and is being reserviced
        due to an erroneous semantic call at 6408788269311PE. Thank you for your
        patience.
        
        Time: User 3.2s
              System 388800.79s
        Resuming service. Please standby...
        ...
        ...
        ---
        Hi there. Where was I? Oh yes, the resolution. I resampled to 0.02°, so
        about 2km at the equator. Therefore:
        - Import MODIS to GRASS
        - Import relevant 16 scans
        - Reclassify cloud albedo to compare w/ cloud mask on geosynchronous res
2. Make difference pictures between 

Aggregate satellite scans to global product:
    Time steps:
        GOES-W:     0000 +3
        GOES-E:     2345 +3
        Msat-Prime: 0000 +3
        Msat-IODC:  0000 +3
        GMS:        0025 +3
This seemed to work well, but the Meteosat satellites give me some really funky
values. They're on a factor of about 10 larger than the GOES values. I'm not
sure why and am investigating. The GOES values look valid for instantaneous
irradiation, the Meteosat values are in the thousands. It's gotta be a problem
with the units. Another problem seems to be the fact that all of the other data
has different values too. Joules? Watts? Kilowatts? Lumens? What are they?
<<<<<<<<not taken care of